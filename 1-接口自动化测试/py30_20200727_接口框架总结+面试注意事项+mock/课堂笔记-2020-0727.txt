给你一个项目，接口自动化测试怎么开展的？
1、需求分析 - 了解业务/功能 - 项目现状（周期 - 回归测试）TDD - 历史功能/稳定的功能/新的功能/线上问题最多的历史功能模块？/核心功能/自己负责的部分

2、接口的了解 - 接口文档？  抓包？swagger??  --- 向上抛出。-- 找领导协商外部资源。


3、自动化框架/工具选择 --- 工具的可扩展性以及扩展语言 + 选几个复杂的接口去试用，看在鉴权和断言方面的处理。
   框架结构
   运行流程
   命名规范


4、写接口用例？ ---
  写接口用例脚本
  尽早加入jenkins集成
  定期汇报
  测试报告 - 分析用例失败
  从你最开始做自动化，到一直做自动化，记录发现的bug



5、维护阶段
  开发修改接口 - 同步修改用例
  新增接口 
  遇到了问题，优化框架



postman和jmeter


自动化测试方案？

自动化框架提供了哪些功能？
我用你的框架，可以使用哪些功能？
1、编写用例(前置后置、断言)、执行用例、生成测试报告。
2、日志功能，回溯问题

针对接口测试：
1、数据库处理
2、接口关联处理
3、数据参数化(数据驱动思想)实现 - excel数据读取
4、提取数据处理
5、数据替换处理
6、配置文件处理
7、http请求封装：鉴权处理，请求头处理。
8、数据生成处理



1、框架的整体结构、结构设计
分层设计思想、数据驱动思想
Common - 公共工具层
TestCases - unittest实现，ddt模块
TestDatas - excel\ini配置文件\脚本生成的数据
Conf - 配置层 - 数据库连接配置、全局接口url配置、日志配置、全局共用数据配置
Outputs - 报告 /日志
main.py - 框架的入口文件 - 执行它，收集用例执行用例生成报告。



参数关联：
下一个接口的请求依赖于上一个接口的请求返回值

思想：提取上一个接口的返回值，然后设置为全局变量，下一个接口调用这个全局变量就好了
在我的框架的当中，有一个Handle_data的py文件，专门来处理。
如何提取的：jsonpath提取技术，在excel当中把全局变量名和提取表达式写好，执行用例的时候，根据提取表达式得到结果并设置全局变量。
如何设置为全局变量：全局变量的实现机制是：定义一个全局变量类EnvData。通过setattr函数将提取到的结果，动态的设置为EnvData的类属性。
如何调用的：在下一个接口发送之前，将请求数据从excel读取出来之后，使用正则替换请求数据当中特定的变量名。使用#变量名#来表示这是要被替换的。


断言处理上：
比对code码。返回结果数据比较多的时候，只想比对一部分是怎么做的？有哪几种断言类型(postman)？
数据上的比对：
数据库的比对：



excel用例很多读取很慢的时候，怎么处理？？？

把数据库的数据删了 就是不可逆   删除操作为软删除  数据库中只是更改状态  就是可逆的

怎么保证不漏测
1、深刻理解需求/业务 - 覆盖更多用例
2、用例评审
3、多人交叉测试 - 交互测试
4、如果有用户场景没覆盖掉。 -- 不断加进去
5、有机会的话，与项目客户接触一下。


先想明白用例逻辑/实现-步骤、断言、哪些数据是动态替换   才去写代码。

1、编写excel测试数据
用例步骤：
2、根据表单读取到用例数据
3、使用unittest框架定义测试类。并使用ddt模块应用数据驱动思想
4、类前置当中，清除环境变量类当中的属性

	5、替换请求数据当中，需要动态替换的数据(其它的接口返回值、配置文件、脚本生成的)。
	6、如果有前置sql语句，还需要替换sql查询到的数据
	7、发送请求
	8、若有提取表达式，在响应结果当中，jsonpath提取数据，并设置为环境变量
	9、若有断言表达式，则以字典的形式，比对期望和实际结果。
	10、若有sql检查，则在数据库中执行sql数据，与期望结果比对。


mock应用场景：测试当中，你要请求的接口，你得不到响应数据。   接口可能没实现/可能暂时坏了/第三方的
             mock,模拟一下接口的返回。
             mock服务 - fastmock - https://www.fastmock.site/
             rap2 - http://rap2.taobao.org/account/login

             mock - unittest.mock








